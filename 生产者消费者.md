#!/usr/bin/env python
# -*- coding: utf-8 -*-
import itertools
import threading
import signal
from boto3.session import Session
from botocore.config import Config as boto3Config
import multiprocessing
from Queue import Queue

class Producer(threading.Thread):
    def __init__(self, config):
        threading.Thread.__init__(self)
        self.config = config
        self.session_producer = Session(self.config["access_key"], self.config["secret_key"])
        self.client_producer = self.session_producer.client('s3', endpoint_url=self.config["endpoint"],
                                        config=boto3Config(connect_timeout=5,
                                                           retries={'max_attempts': 5}))

        self.paginator = self.client_producer.get_paginator('list_objects')
        self.page_iterator = self.paginator.paginate(Bucket="source")

    def run(self):
        global count
        global queue
        global is_exit
        src_bucket = "source"
        for page in self.page_iterator:
            for item in page['Contents']:
                if cond.acquire():
                    if is_exit:  # 每次获取锁之后，先检查全局状态变量
                        cond.notifyAll()  # 退出前必须唤醒其他所有线程
                        cond.release()  # 退出前必须释放锁
                        break
                    if count > 1000:
                        cond.wait()
                    else:
                        print "put ", item['Key'], " into Queue"
                        queue.put(item['Key'])
                        count = count + 1
                        cond.notify()
                    cond.release()
        is_exit = True
        print "Producer run exit"


class Consumer(threading.Thread):
    def __init__(self, config):
        threading.Thread.__init__(self)
        self.config = config
        self.session_consumer = Session(self.config["access_key"], self.config["secret_key"])
        self.client_consumer = self.session_consumer.client('s3', endpoint_url=self.config["endpoint"],
                                                  config=boto3Config(connect_timeout=5,
                                                                     retries={'max_attempts': 5}))
    def run(self):
        global count
        global queue
        global is_exit
        while True:
            if cond.acquire():
                if is_exit and queue.empty():
                    cond.notifyAll()
                    cond.release()
                    break
                if count < 1:
                    cond.wait()
                else:
                    count = count - 1
                    self.val = queue.get()
                    #resp = self.client_consumer.get_object(Bucket="source", Key=self.val, Range="bytes=0-4096");
                    print "upload... ",  self.val
                    resp = self.client_consumer.put_object(Bucket="largedest", Key=self.val, Body="xxxxxxxxxxxxxxxxxxxxxxxxx");
                    print "%s consume %s, status=%s" % (self.name, self.val, str(resp['ResponseMetadata']['HTTPStatusCode']))
                    cond.notify()
                cond.release()
                
        print "%s Consumer run exit" % (self.name, ) 
            
count = 0
queue = Queue(maxsize=1000)
cond = threading.Condition()
is_exit = False #全局变量
def signal_handler(signum, frame): #信号处理函数
    global is_exit
    is_exit = True #主线程信号处理函数修改全局变量，提示子线程退出
    print "Get signal, set is_exit = True"
def test():
    producers = []
    consumers = []
    Pconfig = {"access_key":"eos", "secret_key":"eos","endpoint":"http://10.128.3.68"}
    Cconfig = {"access_key":"eos", "secret_key":"eos","endpoint":"http://10.128.3.68"}
    for i in xrange(1):
        p = Producer(Pconfig)
        producers.append(p)
        p.setDaemon(True) #子线程daemon
        p.start()
    for j in xrange(100):
        c = Consumer(Cconfig)
        consumers.append(c)
        c.setDaemon(True) #子线程daemon
        c.start()
    while 1:
        alive = False
        for t in itertools.chain(producers, consumers): #循环检查所有子线程
            alive = alive or t.isAlive() #保证所有子线程退出
        if not alive:
            break
if __name__ == "__main__":
    signal.signal(signal.SIGINT, signal_handler) #注册信号处理函数
    signal.signal(signal.SIGTERM, signal_handler) #注册信号处理函数
    test()
    
    
    
    
#!/usr/bin/env python
# -*- coding: utf-8 -*-
import itertools
import threading
import signal
from boto3.session import Session
from botocore.config import Config as boto3Config
import multiprocessing
from Queue import Queue
from multiprocessing import Pool

class Producer(threading.Thread):
    def __init__(self, config):
        threading.Thread.__init__(self)
        self.config = config
        self.session_producer = Session(self.config["access_key"], self.config["secret_key"])
        self.client_producer = self.session_producer.client('s3', endpoint_url=self.config["endpoint"],
                                                            config=boto3Config(connect_timeout=5,
                                                                               retries={'max_attempts': 5}))

        self.paginator = self.client_producer.get_paginator('list_objects')
        self.page_iterator = self.paginator.paginate(Bucket="source")

    def run(self):
        global count
        global queue
        global is_exit
        src_bucket = "source"
        for page in self.page_iterator:
            for item in page['Contents']:
                if cond.acquire():
                    if is_exit:  # 每次获取锁之后，先检查全局状态变量
                        cond.notifyAll()  # 退出前必须唤醒其他所有线程
                        cond.release()  # 退出前必须释放锁
                        break
                    if count > 1000:
                        cond.wait()
                    else:
                        print "put ", item['Key'], " into Queue"
                        queue.put(item['Key'])
                        count = count + 1
                        cond.notify()
                    cond.release()
        is_exit = True
        print "Producer run exit"

def generate(response):
    for chunk in iter(lambda: response['Body'].read(5 * 1024 * 1024), b''):
        yield chunk

def upload_part(access_key, secret_key, endpoint, bucketname, keyname, multipart_id, part_num, chunk):
    session_tmp = Session(access_key, secret_key)
    client_tmp = session_tmp.client('s3', endpoint_url=endpoint,
                                                        config=boto3Config(connect_timeout=5,
                                                                           retries={'max_attempts': 5}))
    client_tmp.upload_part(
        Bucket=bucketname,
        Key=keyname,
        PartNumber=part_num,
        UploadId=multipart_id,
        Body=chunk,
    )


class Consumer(threading.Thread):
            
    def __init__(self, config):
        threading.Thread.__init__(self)
        self.config = config
        self.session_consumer = Session(self.config["access_key"], self.config["secret_key"])
        self.client_consumer = self.session_consumer.client('s3', endpoint_url=self.config["endpoint"],
                                                            config=boto3Config(connect_timeout=5,
                                                                               retries={'max_attempts': 5}))
        
        
    def run(self):
        global count
        global queue
        global is_exit
        while True:
            if cond.acquire():
                if is_exit and queue.empty():
                    cond.notifyAll()
                    cond.release()
                    break
                if count < 1:
                    cond.wait()
                else:
                    count = count - 1
                    self.val = queue.get()
                    s3_response = self.client_consumer.get_object(Bucket="source", Key=self.val);
                    print "upload... ", self.val
                    pool = Pool(processes=4)
                    if True:
                        self.multi_part_upload = self.client_consumer.create_multipart_upload(Bucket="largedest",
                                                                                              Key=self.val)
                        try:
                            partcount = 0
                            for part_index, chunk in enumerate(generate(s3_response), start=1):
                                pool.apply_async(upload_part,
                                                 ["eos", "eos", "http://10.128.3.68", "largedest", self.val, self.multi_part_upload['UploadId'],
                                                  part_index, chunk])
                           #     self._upload_part(self.config, "largedest", self.val, self.multi_part_upload['UploadId'],part_index, chunk)

                                partcount += 1
                            pool.close()
                            pool.join()

                            paginator = self.client_consumer.get_paginator('list_parts')
                            part_iterator = paginator.paginate(Bucket="largedest", Key=self.val,
                                                               UploadId=self.multi_part_upload['UploadId'])
                            part_info_dict = {'Parts': []}
                            partcountlist = 0
                            for parts in part_iterator:
                                for part in parts['Parts']:
                                    part_info_dict['Parts'].append({
                                        'PartNumber': part['PartNumber'],
                                        'ETag': part['ETag']
                                    })
                                    partcountlist += 1

                            completed_ctx = {
                                'Bucket': "largedest",
                                'Key': self.val,
                                'UploadId': self.multi_part_upload['UploadId'],
                                'MultipartUpload': part_info_dict
                            }

                            if partcount == partcountlist:
                                self.client_consumer.complete_multipart_upload(**completed_ctx)
                                print "%s consume %s  success" % (self.name, self.val)
        
                            else:
                                self.client_consumer.abort_multipart_upload(Bucket="largedest", Key=self.val,
                                                                            UploadId=self.multi_part_upload['UploadId'])
                                print "%s consume %s  failed 1" % (self.name, self.val)

                        except Exception as e:
                            self.client_consumer.abort_multipart_upload(Bucket="largedest", Key=self.val,
                                                                        UploadId=self.multi_part_upload['UploadId'])
                            print "%s consume %s  failed 2" % (self.name, self.val)
                            print e
                    cond.notify()
                cond.release()

        print "%s Consumer run exit" % (self.name,)


count = 0
queue = Queue(maxsize=1000)
cond = threading.Condition()
is_exit = False  # 全局变量


def signal_handler(signum, frame):  # 信号处理函数
    global is_exit
    is_exit = True  # 主线程信号处理函数修改全局变量，提示子线程退出
    print "Get signal, set is_exit = True"


def test():
    producers = []
    consumers = []
    Pconfig = {"access_key": "eos", "secret_key": "eos", "endpoint": "http://10.128.3.68"}
    Cconfig = {"access_key": "eos", "secret_key": "eos", "endpoint": "http://10.128.3.68"}
    for i in xrange(1):
        p = Producer(Pconfig)
        producers.append(p)
        p.setDaemon(True)  # 子线程daemon
        p.start()
    for j in xrange(50):
        c = Consumer(Cconfig)
        consumers.append(c)
        c.setDaemon(True)  # 子线程daemon
        c.start()
    while 1:
        alive = False
        for t in itertools.chain(producers, consumers):  # 循环检查所有子线程
            alive = alive or t.isAlive()  # 保证所有子线程退出
        if not alive:
            break


if __name__ == "__main__":
    signal.signal(signal.SIGINT, signal_handler)  # 注册信号处理函数
    signal.signal(signal.SIGTERM, signal_handler)  # 注册信号处理函数
    test()
